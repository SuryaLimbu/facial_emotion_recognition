{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries \n",
    "\n",
    "#Pandas is mainly used for data analysis.\n",
    "import pandas as pd \n",
    "\n",
    "#it used for real time computer vision\n",
    "import cv2\n",
    "\n",
    "# allows us to operate on underlying interpreter\n",
    "import sys\n",
    "\n",
    "#array computing\n",
    "import numpy as np\n",
    "\n",
    "#used in estimate the performance of machine lerning algorith \n",
    "from sklearn.model_selection import train_test_split \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv file directory \n",
    "csv_dir ='fer2013.csv'\n",
    "#resize size of image\n",
    "size_of_img = (48,48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35887\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(csv_dir)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which load the csv file and reshape the images into 48*48 image and detect the faces and \n",
    "#stores in faces array and finally returns the faces and emotions array\n",
    "\n",
    "def read_csv():\n",
    "    data = pd.read_csv(csv_dir)\n",
    "    pixels = data['pixels'].tolist()\n",
    "    width, height = size_of_img\n",
    "    faces = []\n",
    "    \n",
    "    for pix_seq in pixels:\n",
    "        face = [int(pix) for pix in pix_seq.split(' ')]\n",
    "        face = np.asarray(face).reshape(width, height)\n",
    "        face = cv2.resize(face.astype('uint8'), size_of_img)\n",
    "        faces.append(face.astype('float32'))\n",
    "        \n",
    "    faces = np.asarray(faces)\n",
    "    faces = np.expand_dims(faces, -1)\n",
    "    emotions = pd.get_dummies(data['emotion']).values\n",
    "        \n",
    "    return faces, emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#currently data in an int8 formate so it need to convert its type to float32 before feed in to the network. \n",
    "#and also rescaele the pixels values in range 0 - 1 .\n",
    "def input_preprocess(x, v2=True):\n",
    "    x = x.astype('float32')\n",
    "    x = x / 255.0\n",
    "    if v2:\n",
    "        x = x - 0.5\n",
    "        x = x * 2.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces, emotions = read_csv()\n",
    "faces = input_preprocess(faces)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(faces,emotions,test_size = 0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images:  (28709, 48, 48, 1) (28709, 7)\n",
      "Test Imgaes:  (7178, 48, 48, 1) (7178, 7)\n"
     ]
    }
   ],
   "source": [
    "print('Train Images: ',xtrain.shape, ytrain.shape)\n",
    "print('Test Imgaes: ',xtest.shape,ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import layers\n",
    "from keras.layers import Activation, Convolution2D, Conv2D, Dropout, AveragePooling2D, BatchNormalization, GlobalAveragePooling2D, Flatten, Input, MaxPooling2D, SeparableConv2D\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "\n",
    "#parameters\n",
    "batch_size = 32 # the num. of samples to eork through before updating the internal model parameters.\n",
    "num_of_epochs = 100 #the num. of times that the learning algorithm will work through the entire training dataset.\n",
    "shape_of_img = (48, 48, 1) #shape of the image to 48 x 48 x 1\n",
    "verbose = True # to see the training progress for each epochs\n",
    "num_of_classes = 7\n",
    "patience = 50\n",
    "base_dir = 'models/' #model directory \n",
    "l2_regularization = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 48, 48, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 46, 46, 8)    72          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 46, 46, 8)    32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 46, 46, 8)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 44, 44, 8)    576         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 44, 44, 8)    32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 44, 44, 8)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d (SeparableConv (None, 44, 44, 16)   200         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 44, 44, 16)   64          separable_conv2d[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 44, 44, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 44, 44, 16)   400         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 44, 44, 16)   64          separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 22, 22, 16)   128         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 22, 22, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 22, 22, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 22, 22, 16)   0           max_pooling2d[0][0]              \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 22, 22, 32)   656         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 22, 22, 32)   128         separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 22, 22, 32)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 22, 22, 32)   1312        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 22, 22, 32)   128         separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 11, 11, 32)   512         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 11, 11, 32)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 11, 11, 32)   128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 11, 11, 32)   0           max_pooling2d_1[0][0]            \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 11, 11, 64)   2336        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 11, 11, 64)   256         separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 11, 11, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, 11, 11, 64)   4672        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 6, 6, 64)     2048        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 6, 6, 64)     0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 6, 6, 64)     256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 6, 6, 64)     0           max_pooling2d_2[0][0]            \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCo (None, 6, 6, 128)    8768        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 6, 6, 128)    0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCo (None, 6, 6, 128)    17536       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 3, 3, 128)    8192        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 3, 3, 128)    0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 3, 3, 128)    512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 3, 3, 128)    0           max_pooling2d_3[0][0]            \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 3, 3, 7)      8071        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 7)            0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Activation)        (None, 7)            0           global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 58,423\n",
      "Trainable params: 56,951\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#data generator\n",
    "data_generator = ImageDataGenerator(featurewise_center = False,featurewise_std_normalization = False,\n",
    "                                    rotation_range = 10, width_shift_range = 0.1, height_shift_range = 0.1,\n",
    "                                    zoom_range = .1, horizontal_flip = True)\n",
    "#model parameters \n",
    "regularization = l2(l2_regularization)\n",
    "\n",
    "# base\n",
    "image_input = Input(shape_of_img)\n",
    "x = Conv2D(filters=8, kernel_size=(3,3), strides=(1,1), kernel_regularizer=regularization, use_bias=False)(image_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(filters=8, kernel_size=(3,3), strides=(1,1), kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "# module 1\n",
    "# residual module \n",
    "residual = Conv2D(filters=16, kernel_size=(1,1), strides=(2,2), padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "\n",
    "x = SeparableConv2D(filters=16, kernel_size=(3,3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(filters=16, kernel_size=(3,3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same')(x)\n",
    "x = layers.add([x,residual])\n",
    "\n",
    "# module 2\n",
    "# residual module \n",
    "residual = Conv2D(filters=32, kernel_size=(1,1), strides=(2,2), padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "\n",
    "x = SeparableConv2D(filters=32, kernel_size=(3,3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(filters=32, kernel_size=(3,3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same')(x)\n",
    "x = layers.add([x,residual])\n",
    "\n",
    "# module 3\n",
    "# residual module \n",
    "residual = Conv2D(filters=64, kernel_size=(1,1), strides=(2,2), padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "\n",
    "x = SeparableConv2D(filters=64, kernel_size=(3,3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(filters=64, kernel_size=(3,3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same')(x)\n",
    "x = layers.add([x,residual])\n",
    "\n",
    "# module 4\n",
    "# residual module \n",
    "residual = Conv2D(filters=128, kernel_size=(1,1), strides=(2,2), padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "\n",
    "x = SeparableConv2D(filters=128, kernel_size=(3,3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(filters=128, kernel_size=(3,3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same')(x)\n",
    "x = layers.add([x,residual])\n",
    "\n",
    "x = Conv2D(filters=num_of_classes, kernel_size=(3,3), padding='same')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "output = Activation('softmax', name='predictions')(x)\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "model = Model(image_input, output)\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## need to uncomment to train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "897/897 [==============================] - 243s 268ms/step - loss: 1.8974 - accuracy: 0.2811 - val_loss: 1.6430 - val_accuracy: 0.4069\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.64299, saving model to models\\_mini_XCEPTION.01-0.41.hdf5\n",
      "Epoch 2/100\n",
      "897/897 [==============================] - 278s 309ms/step - loss: 1.5652 - accuracy: 0.4161 - val_loss: 1.4953 - val_accuracy: 0.4408\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.64299 to 1.49528, saving model to models\\_mini_XCEPTION.02-0.44.hdf5\n",
      "Epoch 3/100\n",
      "897/897 [==============================] - 268s 299ms/step - loss: 1.4441 - accuracy: 0.4572 - val_loss: 1.5074 - val_accuracy: 0.4557\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.49528\n",
      "Epoch 4/100\n",
      "897/897 [==============================] - 251s 279ms/step - loss: 1.3419 - accuracy: 0.4974 - val_loss: 1.5246 - val_accuracy: 0.4869\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.49528\n",
      "Epoch 5/100\n",
      "897/897 [==============================] - 240s 268ms/step - loss: 1.2904 - accuracy: 0.5166 - val_loss: 1.3736 - val_accuracy: 0.4748\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.49528 to 1.37357, saving model to models\\_mini_XCEPTION.05-0.47.hdf5\n",
      "Epoch 6/100\n",
      "897/897 [==============================] - 235s 262ms/step - loss: 1.2642 - accuracy: 0.5281 - val_loss: 1.2568 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.37357 to 1.25685, saving model to models\\_mini_XCEPTION.06-0.54.hdf5\n",
      "Epoch 7/100\n",
      "897/897 [==============================] - 237s 265ms/step - loss: 1.2214 - accuracy: 0.5463 - val_loss: 1.3216 - val_accuracy: 0.5124\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.25685\n",
      "Epoch 8/100\n",
      "897/897 [==============================] - 251s 279ms/step - loss: 1.2117 - accuracy: 0.5468 - val_loss: 1.2184 - val_accuracy: 0.5389\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.25685 to 1.21840, saving model to models\\_mini_XCEPTION.08-0.54.hdf5\n",
      "Epoch 9/100\n",
      "897/897 [==============================] - 256s 285ms/step - loss: 1.1833 - accuracy: 0.5543 - val_loss: 1.3475 - val_accuracy: 0.4992\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.21840\n",
      "Epoch 10/100\n",
      "897/897 [==============================] - 285s 317ms/step - loss: 1.1682 - accuracy: 0.5648 - val_loss: 1.2182 - val_accuracy: 0.5488\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.21840 to 1.21823, saving model to models\\_mini_XCEPTION.10-0.55.hdf5\n",
      "Epoch 11/100\n",
      "897/897 [==============================] - 298s 332ms/step - loss: 1.1614 - accuracy: 0.5720 - val_loss: 1.2354 - val_accuracy: 0.5433\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.21823\n",
      "Epoch 12/100\n",
      "897/897 [==============================] - 266s 297ms/step - loss: 1.1401 - accuracy: 0.5699 - val_loss: 1.2748 - val_accuracy: 0.5382\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.21823\n",
      "Epoch 13/100\n",
      "897/897 [==============================] - 254s 283ms/step - loss: 1.1319 - accuracy: 0.5841 - val_loss: 1.2196 - val_accuracy: 0.5524\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.21823\n",
      "Epoch 14/100\n",
      "897/897 [==============================] - 253s 282ms/step - loss: 1.1079 - accuracy: 0.5840 - val_loss: 1.2222 - val_accuracy: 0.5415\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.21823\n",
      "Epoch 15/100\n",
      "897/897 [==============================] - 229s 255ms/step - loss: 1.1016 - accuracy: 0.5881 - val_loss: 1.1557 - val_accuracy: 0.5701\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.21823 to 1.15567, saving model to models\\_mini_XCEPTION.15-0.57.hdf5\n",
      "Epoch 16/100\n",
      "897/897 [==============================] - 221s 247ms/step - loss: 1.1069 - accuracy: 0.5821 - val_loss: 1.1549 - val_accuracy: 0.5706\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.15567 to 1.15492, saving model to models\\_mini_XCEPTION.16-0.57.hdf5\n",
      "Epoch 17/100\n",
      "897/897 [==============================] - 262s 292ms/step - loss: 1.0908 - accuracy: 0.5886 - val_loss: 1.1702 - val_accuracy: 0.5652\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.15492\n",
      "Epoch 18/100\n",
      "897/897 [==============================] - 272s 303ms/step - loss: 1.0857 - accuracy: 0.5951 - val_loss: 1.1624 - val_accuracy: 0.5680\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.15492\n",
      "Epoch 19/100\n",
      "897/897 [==============================] - 268s 298ms/step - loss: 1.0755 - accuracy: 0.5993 - val_loss: 1.2004 - val_accuracy: 0.5570\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.15492\n",
      "Epoch 20/100\n",
      "897/897 [==============================] - 264s 294ms/step - loss: 1.0800 - accuracy: 0.5953 - val_loss: 1.1419 - val_accuracy: 0.5814\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.15492 to 1.14189, saving model to models\\_mini_XCEPTION.20-0.58.hdf5\n",
      "Epoch 21/100\n",
      "897/897 [==============================] - 218s 243ms/step - loss: 1.0572 - accuracy: 0.6061 - val_loss: 1.1839 - val_accuracy: 0.5777\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.14189\n",
      "Epoch 22/100\n",
      "897/897 [==============================] - 289s 322ms/step - loss: 1.0506 - accuracy: 0.6055 - val_loss: 1.1118 - val_accuracy: 0.5876\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.14189 to 1.11178, saving model to models\\_mini_XCEPTION.22-0.59.hdf5\n",
      "Epoch 23/100\n",
      "897/897 [==============================] - 331s 369ms/step - loss: 1.0395 - accuracy: 0.6071 - val_loss: 1.1666 - val_accuracy: 0.5683\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.11178\n",
      "Epoch 24/100\n",
      "897/897 [==============================] - 307s 343ms/step - loss: 1.0411 - accuracy: 0.6098 - val_loss: 1.1556 - val_accuracy: 0.5744\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.11178\n",
      "Epoch 25/100\n",
      "897/897 [==============================] - 269s 300ms/step - loss: 1.0422 - accuracy: 0.6142 - val_loss: 1.1091 - val_accuracy: 0.5933\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.11178 to 1.10906, saving model to models\\_mini_XCEPTION.25-0.59.hdf5\n",
      "Epoch 26/100\n",
      "897/897 [==============================] - 216s 241ms/step - loss: 1.0254 - accuracy: 0.6182 - val_loss: 1.1524 - val_accuracy: 0.5784\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.10906\n",
      "Epoch 27/100\n",
      "897/897 [==============================] - 216s 241ms/step - loss: 1.0141 - accuracy: 0.6252 - val_loss: 1.1696 - val_accuracy: 0.5777\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.10906\n",
      "Epoch 28/100\n",
      "897/897 [==============================] - 216s 240ms/step - loss: 1.0154 - accuracy: 0.6167 - val_loss: 1.0741 - val_accuracy: 0.6004\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.10906 to 1.07413, saving model to models\\_mini_XCEPTION.28-0.60.hdf5\n",
      "Epoch 29/100\n",
      "897/897 [==============================] - 215s 240ms/step - loss: 1.0072 - accuracy: 0.6242 - val_loss: 1.0719 - val_accuracy: 0.6056\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.07413 to 1.07193, saving model to models\\_mini_XCEPTION.29-0.61.hdf5\n",
      "Epoch 30/100\n",
      "897/897 [==============================] - 216s 240ms/step - loss: 1.0136 - accuracy: 0.6230 - val_loss: 1.0750 - val_accuracy: 0.6184\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.07193\n",
      "Epoch 31/100\n",
      "897/897 [==============================] - 215s 240ms/step - loss: 1.0063 - accuracy: 0.6244 - val_loss: 1.0615 - val_accuracy: 0.6074\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.07193 to 1.06149, saving model to models\\_mini_XCEPTION.31-0.61.hdf5\n",
      "Epoch 32/100\n",
      "897/897 [==============================] - 215s 240ms/step - loss: 0.9956 - accuracy: 0.6300 - val_loss: 1.1146 - val_accuracy: 0.5854\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.06149\n",
      "Epoch 33/100\n",
      "897/897 [==============================] - 216s 240ms/step - loss: 0.9960 - accuracy: 0.6281 - val_loss: 1.0809 - val_accuracy: 0.6055\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.06149\n",
      "Epoch 34/100\n",
      "897/897 [==============================] - 216s 240ms/step - loss: 1.0007 - accuracy: 0.6254 - val_loss: 1.1199 - val_accuracy: 0.5871\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.06149\n",
      "Epoch 35/100\n",
      "897/897 [==============================] - 215s 240ms/step - loss: 0.9889 - accuracy: 0.6301 - val_loss: 1.0868 - val_accuracy: 0.6067\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.06149\n",
      "Epoch 36/100\n",
      "897/897 [==============================] - 216s 240ms/step - loss: 0.9851 - accuracy: 0.6340 - val_loss: 1.1310 - val_accuracy: 0.5801\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.06149\n",
      "Epoch 37/100\n",
      "897/897 [==============================] - 216s 240ms/step - loss: 0.9962 - accuracy: 0.6285 - val_loss: 1.0848 - val_accuracy: 0.6028\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.06149\n",
      "Epoch 38/100\n",
      "897/897 [==============================] - 215s 239ms/step - loss: 0.9729 - accuracy: 0.6400 - val_loss: 1.2458 - val_accuracy: 0.5595\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.06149\n",
      "Epoch 39/100\n",
      "897/897 [==============================] - 215s 239ms/step - loss: 0.9755 - accuracy: 0.6363 - val_loss: 1.1020 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.06149\n",
      "Epoch 40/100\n",
      "897/897 [==============================] - 215s 239ms/step - loss: 0.9741 - accuracy: 0.6337 - val_loss: 1.0890 - val_accuracy: 0.6013\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.06149\n",
      "Epoch 41/100\n",
      "897/897 [==============================] - 214s 239ms/step - loss: 0.9718 - accuracy: 0.6374 - val_loss: 1.0859 - val_accuracy: 0.5946\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.06149\n",
      "Epoch 42/100\n",
      "897/897 [==============================] - 215s 239ms/step - loss: 0.9767 - accuracy: 0.6365 - val_loss: 1.0536 - val_accuracy: 0.6106\n",
      "\n",
      "Epoch 00042: val_loss improved from 1.06149 to 1.05365, saving model to models\\_mini_XCEPTION.42-0.61.hdf5\n",
      "Epoch 43/100\n",
      "897/897 [==============================] - 215s 239ms/step - loss: 0.9605 - accuracy: 0.6396 - val_loss: 1.0984 - val_accuracy: 0.5956\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.05365\n",
      "Epoch 44/100\n",
      "897/897 [==============================] - 214s 239ms/step - loss: 0.9608 - accuracy: 0.6435 - val_loss: 1.0858 - val_accuracy: 0.5978\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.05365\n",
      "Epoch 45/100\n",
      "897/897 [==============================] - 215s 239ms/step - loss: 0.9550 - accuracy: 0.6394 - val_loss: 1.1446 - val_accuracy: 0.5992\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.05365\n",
      "Epoch 46/100\n",
      "897/897 [==============================] - 215s 239ms/step - loss: 0.9549 - accuracy: 0.6428 - val_loss: 1.1005 - val_accuracy: 0.6080\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.05365\n",
      "Epoch 47/100\n",
      "897/897 [==============================] - 215s 239ms/step - loss: 0.9534 - accuracy: 0.6419 - val_loss: 1.0697 - val_accuracy: 0.6123\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.05365\n",
      "Epoch 48/100\n",
      "897/897 [==============================] - 215s 239ms/step - loss: 0.9382 - accuracy: 0.6495 - val_loss: 1.0408 - val_accuracy: 0.6162\n",
      "\n",
      "Epoch 00048: val_loss improved from 1.05365 to 1.04083, saving model to models\\_mini_XCEPTION.48-0.62.hdf5\n",
      "Epoch 49/100\n",
      "897/897 [==============================] - 215s 239ms/step - loss: 0.9589 - accuracy: 0.6416 - val_loss: 1.0655 - val_accuracy: 0.6037\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.04083\n",
      "Epoch 50/100\n",
      "897/897 [==============================] - 215s 239ms/step - loss: 0.9493 - accuracy: 0.6498 - val_loss: 1.0974 - val_accuracy: 0.5958\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.04083\n",
      "Epoch 51/100\n",
      "897/897 [==============================] - 220s 245ms/step - loss: 0.9541 - accuracy: 0.6436 - val_loss: 1.0495 - val_accuracy: 0.6123\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.04083\n",
      "Epoch 52/100\n",
      "897/897 [==============================] - 272s 303ms/step - loss: 0.9395 - accuracy: 0.6489 - val_loss: 1.0748 - val_accuracy: 0.6041\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.04083\n",
      "Epoch 53/100\n",
      "897/897 [==============================] - 271s 302ms/step - loss: 0.9352 - accuracy: 0.6527 - val_loss: 1.0474 - val_accuracy: 0.6082\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.04083\n",
      "Epoch 54/100\n",
      "897/897 [==============================] - 216s 241ms/step - loss: 0.9472 - accuracy: 0.6451 - val_loss: 1.1154 - val_accuracy: 0.6018\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.04083\n",
      "Epoch 55/100\n",
      "897/897 [==============================] - 215s 240ms/step - loss: 0.9291 - accuracy: 0.6525 - val_loss: 1.0491 - val_accuracy: 0.6232\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.04083\n",
      "Epoch 56/100\n",
      "897/897 [==============================] - 215s 240ms/step - loss: 0.9306 - accuracy: 0.6539 - val_loss: 1.1102 - val_accuracy: 0.6013\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.04083\n",
      "Epoch 57/100\n",
      "897/897 [==============================] - 216s 241ms/step - loss: 0.9414 - accuracy: 0.6494 - val_loss: 1.0770 - val_accuracy: 0.6074\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.04083\n",
      "Epoch 58/100\n",
      "897/897 [==============================] - 243s 271ms/step - loss: 0.9209 - accuracy: 0.6560 - val_loss: 1.0606 - val_accuracy: 0.6194\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.04083\n",
      "Epoch 59/100\n",
      "897/897 [==============================] - 251s 280ms/step - loss: 0.9273 - accuracy: 0.6561 - val_loss: 1.0660 - val_accuracy: 0.6216\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.04083\n",
      "Epoch 60/100\n",
      "897/897 [==============================] - 259s 289ms/step - loss: 0.9284 - accuracy: 0.6546 - val_loss: 1.0279 - val_accuracy: 0.6181\n",
      "\n",
      "Epoch 00060: val_loss improved from 1.04083 to 1.02787, saving model to models\\_mini_XCEPTION.60-0.62.hdf5\n",
      "Epoch 61/100\n",
      "897/897 [==============================] - 252s 281ms/step - loss: 0.9291 - accuracy: 0.6558 - val_loss: 1.0282 - val_accuracy: 0.6298\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.02787\n",
      "Epoch 62/100\n",
      "897/897 [==============================] - 257s 286ms/step - loss: 0.9249 - accuracy: 0.6543 - val_loss: 1.0617 - val_accuracy: 0.6227\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.02787\n",
      "Epoch 63/100\n",
      "897/897 [==============================] - 274s 305ms/step - loss: 0.9273 - accuracy: 0.6609 - val_loss: 1.0401 - val_accuracy: 0.6201\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.02787\n",
      "Epoch 64/100\n",
      "897/897 [==============================] - 261s 291ms/step - loss: 0.9323 - accuracy: 0.6570 - val_loss: 1.0414 - val_accuracy: 0.6213\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.02787\n",
      "Epoch 65/100\n",
      "897/897 [==============================] - 297s 331ms/step - loss: 0.9211 - accuracy: 0.6541 - val_loss: 1.0705 - val_accuracy: 0.6084\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.02787\n",
      "Epoch 66/100\n",
      "897/897 [==============================] - 292s 326ms/step - loss: 0.9095 - accuracy: 0.6606 - val_loss: 1.0557 - val_accuracy: 0.6193\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.02787\n",
      "Epoch 67/100\n",
      "897/897 [==============================] - 261s 291ms/step - loss: 0.9074 - accuracy: 0.6648 - val_loss: 1.0359 - val_accuracy: 0.6159\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.02787\n",
      "Epoch 68/100\n",
      "897/897 [==============================] - 254s 284ms/step - loss: 0.9158 - accuracy: 0.6592 - val_loss: 1.0793 - val_accuracy: 0.6204\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.02787\n",
      "Epoch 69/100\n",
      "897/897 [==============================] - 252s 281ms/step - loss: 0.9128 - accuracy: 0.6607 - val_loss: 1.0524 - val_accuracy: 0.6184\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.02787\n",
      "Epoch 70/100\n",
      "897/897 [==============================] - 252s 281ms/step - loss: 0.9079 - accuracy: 0.6596 - val_loss: 1.0379 - val_accuracy: 0.6265\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.02787\n",
      "Epoch 71/100\n",
      "897/897 [==============================] - 253s 282ms/step - loss: 0.8962 - accuracy: 0.6689 - val_loss: 1.0500 - val_accuracy: 0.6144\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.02787\n",
      "Epoch 72/100\n",
      "897/897 [==============================] - 253s 282ms/step - loss: 0.9063 - accuracy: 0.6607 - val_loss: 1.0140 - val_accuracy: 0.6314\n",
      "\n",
      "Epoch 00072: val_loss improved from 1.02787 to 1.01401, saving model to models\\_mini_XCEPTION.72-0.63.hdf5\n",
      "Epoch 73/100\n",
      "897/897 [==============================] - 259s 288ms/step - loss: 0.8964 - accuracy: 0.6692 - val_loss: 1.0412 - val_accuracy: 0.6233\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.01401\n",
      "Epoch 74/100\n",
      "897/897 [==============================] - 251s 280ms/step - loss: 0.9047 - accuracy: 0.6662 - val_loss: 1.0874 - val_accuracy: 0.6110\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.01401\n",
      "Epoch 75/100\n",
      "897/897 [==============================] - 238s 265ms/step - loss: 0.9052 - accuracy: 0.6638 - val_loss: 1.1010 - val_accuracy: 0.6070\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.01401\n",
      "Epoch 76/100\n",
      "897/897 [==============================] - 216s 241ms/step - loss: 0.8929 - accuracy: 0.6649 - val_loss: 1.0356 - val_accuracy: 0.6244\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.01401\n",
      "Epoch 77/100\n",
      "897/897 [==============================] - 216s 240ms/step - loss: 0.9045 - accuracy: 0.6660 - val_loss: 1.0649 - val_accuracy: 0.6227\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.01401\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "897/897 [==============================] - 215s 239ms/step - loss: 0.9040 - accuracy: 0.6658 - val_loss: 1.0304 - val_accuracy: 0.6265\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.01401\n",
      "Epoch 79/100\n",
      "897/897 [==============================] - 215s 240ms/step - loss: 0.8968 - accuracy: 0.6644 - val_loss: 1.0951 - val_accuracy: 0.6020\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.01401\n",
      "Epoch 80/100\n",
      "897/897 [==============================] - 216s 240ms/step - loss: 0.8928 - accuracy: 0.6697 - val_loss: 1.0865 - val_accuracy: 0.6155\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.01401\n",
      "Epoch 81/100\n",
      "897/897 [==============================] - 215s 240ms/step - loss: 0.8875 - accuracy: 0.6698 - val_loss: 1.0456 - val_accuracy: 0.6255\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.01401\n",
      "Epoch 82/100\n",
      "897/897 [==============================] - 216s 240ms/step - loss: 0.8901 - accuracy: 0.6728 - val_loss: 1.0481 - val_accuracy: 0.6254\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.01401\n",
      "Epoch 83/100\n",
      "897/897 [==============================] - 215s 240ms/step - loss: 0.8851 - accuracy: 0.6719 - val_loss: 1.0508 - val_accuracy: 0.6142\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.01401\n",
      "Epoch 84/100\n",
      "897/897 [==============================] - 215s 240ms/step - loss: 0.8778 - accuracy: 0.6728 - val_loss: 1.0467 - val_accuracy: 0.6222\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.01401\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 85/100\n",
      "897/897 [==============================] - 265s 296ms/step - loss: 0.8525 - accuracy: 0.6836 - val_loss: 1.0026 - val_accuracy: 0.6402\n",
      "\n",
      "Epoch 00085: val_loss improved from 1.01401 to 1.00256, saving model to models\\_mini_XCEPTION.85-0.64.hdf5\n",
      "Epoch 86/100\n",
      "897/897 [==============================] - 272s 303ms/step - loss: 0.8358 - accuracy: 0.6938 - val_loss: 0.9936 - val_accuracy: 0.6438\n",
      "\n",
      "Epoch 00086: val_loss improved from 1.00256 to 0.99362, saving model to models\\_mini_XCEPTION.86-0.64.hdf5\n",
      "Epoch 87/100\n",
      "897/897 [==============================] - 272s 303ms/step - loss: 0.8334 - accuracy: 0.6927 - val_loss: 0.9913 - val_accuracy: 0.6425\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.99362 to 0.99133, saving model to models\\_mini_XCEPTION.87-0.64.hdf5\n",
      "Epoch 88/100\n",
      "897/897 [==============================] - 3372s 4s/step - loss: 0.8308 - accuracy: 0.6930 - val_loss: 0.9956 - val_accuracy: 0.6432\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.99133\n",
      "Epoch 89/100\n",
      "897/897 [==============================] - 281s 314ms/step - loss: 0.8213 - accuracy: 0.6979 - val_loss: 0.9924 - val_accuracy: 0.6438\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.99133\n",
      "Epoch 90/100\n",
      "897/897 [==============================] - 288s 322ms/step - loss: 0.8152 - accuracy: 0.6976 - val_loss: 0.9922 - val_accuracy: 0.6413\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.99133\n",
      "Epoch 91/100\n",
      "897/897 [==============================] - 282s 314ms/step - loss: 0.8168 - accuracy: 0.7000 - val_loss: 0.9979 - val_accuracy: 0.6407\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.99133\n",
      "Epoch 92/100\n",
      "897/897 [==============================] - 279s 311ms/step - loss: 0.8209 - accuracy: 0.6948 - val_loss: 0.9938 - val_accuracy: 0.6431\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.99133\n",
      "Epoch 93/100\n",
      "897/897 [==============================] - 348s 387ms/step - loss: 0.8123 - accuracy: 0.6997 - val_loss: 0.9973 - val_accuracy: 0.6408\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.99133\n",
      "Epoch 94/100\n",
      "897/897 [==============================] - 286s 318ms/step - loss: 0.8075 - accuracy: 0.7014 - val_loss: 0.9926 - val_accuracy: 0.6413\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.99133\n",
      "Epoch 95/100\n",
      "897/897 [==============================] - 289s 322ms/step - loss: 0.8139 - accuracy: 0.6995 - val_loss: 0.9876 - val_accuracy: 0.6411\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.99133 to 0.98762, saving model to models\\_mini_XCEPTION.95-0.64.hdf5\n",
      "Epoch 96/100\n",
      "897/897 [==============================] - 329s 367ms/step - loss: 0.8087 - accuracy: 0.6997 - val_loss: 0.9931 - val_accuracy: 0.6407\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.98762\n",
      "Epoch 97/100\n",
      "897/897 [==============================] - 329s 367ms/step - loss: 0.8065 - accuracy: 0.7002 - val_loss: 0.9917 - val_accuracy: 0.6404\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.98762\n",
      "Epoch 98/100\n",
      "897/897 [==============================] - 312s 348ms/step - loss: 0.8213 - accuracy: 0.6967 - val_loss: 0.9997 - val_accuracy: 0.6397\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.98762\n",
      "Epoch 99/100\n",
      "897/897 [==============================] - 314s 350ms/step - loss: 0.8118 - accuracy: 0.7026 - val_loss: 0.9942 - val_accuracy: 0.6388\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.98762\n",
      "Epoch 100/100\n",
      "897/897 [==============================] - 285s 318ms/step - loss: 0.8169 - accuracy: 0.6914 - val_loss: 0.9872 - val_accuracy: 0.6439\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.98762 to 0.98716, saving model to models\\_mini_XCEPTION.100-0.64.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13d53eb13d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# callbacks\n",
    "''''log_file_path = base_dir + '_emotion_training.log'\n",
    "csv_logger = CSVLogger(log_file_path, append=False)\n",
    "early_stop = EarlyStopping('val_loss', patience=patience)\n",
    "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)\n",
    "trained_models_path = base_dir + '_mini_XCEPTION'\n",
    "model_names = trained_models_path + '.{epoch:02d}-{val_accuracy:.2f}.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,save_best_only=True)\n",
    "callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]\n",
    " \n",
    "model.fit_generator(data_generator.flow(xtrain, ytrain,batch_size),\n",
    "                        steps_per_epoch=len(xtrain) / batch_size,\n",
    "                        epochs=num_of_epochs, verbose=1, callbacks=callbacks,\n",
    "                        validation_data=(xtest,ytest))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To save the train model log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: train_model.h5py\\assets\n"
     ]
    }
   ],
   "source": [
    "#save the train model\n",
    "#next time we do not need to train above train files again\n",
    "#it will continue from above save model\n",
    "#model.save(\"train_model.h5py\") #uncomment to save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import load_model\n",
    "import imutils\n",
    "import cv2\n",
    "import sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'models/'\n",
    "detection_model_path = 'haarcascade/haarcascade_frontalface_default.xml'\n",
    "emotion_recognition_model_path = base_dir + '_mini_XCEPTION.100-0.64.hdf5'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection = cv2.CascadeClassifier(detection_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_classifier = load_model(emotion_recognition_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['angry', 'disgust', 'scared', 'happy', 'sad', 'surprised', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion dectection through web cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.namedWindow('facial_emotion_recognition')\n",
    "camera = cv2.VideoCapture(0)  ## uncomment to use your laptop camera \n",
    "#camera = cv2.VideoCapture('various_emotions.mp4')  # uncomment to read from a video file\n",
    "\n",
    "sz = (int(camera.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "        int(camera.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mpeg')\n",
    "\n",
    "out = cv2.VideoWriter()\n",
    "out.open('output_various_facial_emotions.mp4',fourcc, 15, sz, True) # initialize the writer\n",
    "\n",
    "\n",
    "# while True: # when reading from a video camera, use this while condition\n",
    "while(camera.read()[0]):  # when reading from a video file, use this while condition\n",
    "    color_frame = camera.read()[1]\n",
    "    color_frame = imutils.resize(color_frame,width=min(720, color_frame.shape[1]))\n",
    "    \n",
    "    \n",
    "    gray_frame = cv2.cvtColor(color_frame, cv2.COLOR_BGR2GRAY)\n",
    "    detected_faces = face_detection.detectMultiScale(gray_frame,scaleFactor=1.1,minNeighbors=5,minSize=(30,30),flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    \n",
    "    \n",
    "    canvas = np.zeros((250, 300, 3), dtype=\"uint8\")\n",
    "    frameClone = color_frame.copy()    \n",
    "\n",
    "    \n",
    "    if len(detected_faces)>0:\n",
    "\n",
    "        detected_faces = sorted(detected_faces, reverse=True, key=lambda x: (x[2]-x[0])*(x[3]-x[1]))[0] # if more than one faces\n",
    "        (fx, fy, fw, fh) = detected_faces\n",
    "\n",
    "        im = gray_frame[fy:fy+fh, fx:fx+fw]\n",
    "        im = cv2.resize(im, (48,48))  # the model is trained on 48*48 pixel image \n",
    "        im = im.astype(\"float\")/255.0\n",
    "        im = img_to_array(im)\n",
    "        im = np.expand_dims(im, axis=0)\n",
    "\n",
    "        preds = emotion_classifier.predict(im)[0]\n",
    "        emotion_probability = np.max(preds)\n",
    "        label = emotions[preds.argmax()]\n",
    "\n",
    "        cv2.putText(color_frame, label, (fx, fy-10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "        cv2.rectangle(color_frame, (fx, fy), (fx + fw, fy + fh),(0, 0, 255), 2)\n",
    "\n",
    "    \n",
    "    for (i, (emotion, prob)) in enumerate(zip(emotions, preds)):\n",
    "        # construct the label text\n",
    "        text = \"{}: {:.2f}%\".format(emotion, prob * 100)\n",
    "        w = int(prob * 300)\n",
    "        \n",
    "        cv2.rectangle(canvas, (7, (i * 35) + 5), (w, (i * 35) + 35), (0, 50, 100), -1)\n",
    "        cv2.putText(canvas, text, (10, (i * 35) + 23), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 255, 255), 1)\n",
    "        cv2.putText(frameClone, label, (fx, fy - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (100, 150, 100), 2)\n",
    "        cv2.rectangle(frameClone, (fx, fy), (fx + fw, fy + fh), (100, 100, 100), 2)\n",
    "    \n",
    "    out.write(frameClone)\n",
    "    out.write(canvas)\n",
    "    \n",
    "    cv2.imshow('emotion_recognition', frameClone)\n",
    "    cv2.imshow(\"Probabilities\", canvas)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "camera.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion Dection through Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:7: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-23-3b39de3629c0>:7: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if faces is ():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of face detected:  24\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/priya-dwivedi/face_and_emotion_detection/blob/master/src/EmotionDetector_v2.ipynb\n",
    "def face_detector(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img.copy(),cv2.COLOR_BGR2GRAY)\n",
    "    #detect the face from image\n",
    "    faces = face_detection.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
    "    \n",
    "    allfaces = []   \n",
    "    rects = []\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "        allfaces.append(roi_gray)\n",
    "        rects.append((x,w,y,h))\n",
    "    return rects, allfaces, img\n",
    "\n",
    "img = cv2.imread(\"faces.jpg\")\n",
    "rects, faces, image = face_detector(img)\n",
    "print('Number of face detected: ',len(faces))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for face in faces:\n",
    "    roi = face.astype(\"float\") / 255.0\n",
    "    roi = img_to_array(roi)\n",
    "    roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "    # make a prediction on the ROI, then lookup the class\n",
    "    predictions = emotion_classifier.predict(roi)[0]\n",
    "    label = emotions[predictions.argmax()]   \n",
    "\n",
    "    #Overlay our detected emotion on our pic\n",
    "    position_of_labels = (rects[i][0] + int((rects[i][1]/10)), abs(rects[i][2] - 10))\n",
    "    i += 1\n",
    "    cv2.putText(image, label, position_of_labels , cv2.FONT_HERSHEY_SIMPLEX,0.8, (100,255,0), 1)\n",
    "    \n",
    "cv2.imshow(\"Facial Emotion Recogniser\", image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
